Using /home/pozdn/.cache/torch_extensions/py38_cu111 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/pozdn/.cache/torch_extensions/py38_cu111/sparse_accumulation_cuda/build.ninja...
Building extension module sparse_accumulation_cuda...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module sparse_accumulation_cuda...
transformation rule is computed
L_MAX=5; BATCH_SIZE=10000; N_FEATURES=200; sparse dim length = 11; sparse indices length = 126
preparing real life transformation rule
*************
CUDA
*************
***forward***
python loops; active dim 0; forward; cuda:  0.025215061399671765
torch index_add_; active dim 0; forward; cuda:  0.026948707580566406

python loops; active dim 1; forward; cuda:  0.03350805706448025
torch index_add_; active dim 1; forward; cuda:  0.02625405163235135

python loops; active dim 2; forward; cuda:  0.19782067362467448
torch index_add_; active dim 2; forward; cuda:  0.02156950018141005
dense matrix multiply:  0.007511555565728082
sparse matrix multiply; active dim 2; forward; cuda:  0.17568610127766926
sparse matrix optimized multiply; active dim 0; forward; cuda:  0.2286965043809679
CUDA kernel; active dim 2; forward; cuda:  0.0018692764573627048
***backward***
python loops; active dim 0; backward; cuda:  0.4620384724934896
torch index_add_; active dim 0; backward; cuda:  0.03216430155436198

python loops; active dim 1; backward; cuda:  0.5971531100802951
torch index_add_; active dim 1; backward; cuda:  0.035279142591688364

python loops; active dim 2; backward; cuda:  0.309046383327908
torch index_add_; active dim 2; backward; cuda:  0.3086910502115885
CUDA kernel; active dim 2; backward; cuda:  0.004007651567459107
dense matrix multiply:  0.023251587549845378
