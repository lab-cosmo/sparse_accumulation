{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507700e8",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e8e17",
   "metadata": {},
   "source": [
    "[Understood that there is no sense to update tutorial further until certain code development]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfb72c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sparse_accumulation import accumulate\n",
    "from sparse_accumulation import get_cg_transformation_rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9059993",
   "metadata": {},
   "source": [
    "## Preparing a dummy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e5b89",
   "metadata": {},
   "source": [
    "Let's prepare some dummy data to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedcfb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = torch.randn(10, 20, 3)\n",
    "X2 = torch.randn(10, 20, 4)\n",
    "\n",
    "m1 = torch.LongTensor([0, 1, 1, 2])\n",
    "m2 = torch.LongTensor([0, 0, 3, 1])\n",
    "mu = torch.LongTensor([0, 3, 1, 2])\n",
    "\n",
    "C = torch.FloatTensor([0.17, 0.23, 0.4, -0.9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8a702",
   "metadata": {},
   "source": [
    "**Important** sparse accumulation operation requires mu tensor to be sorted to work correctly.\n",
    "\n",
    "It is very clear that the result of the sparse accumulation operation doesn't change for the simultaneous permutation of all the tensors m1, m2, mu, and C since the result of the summation doesn't depend on the order of the terms. Thus, it is always reachable to have mu tensor to be sorted, and one can achieve this as simply as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73214b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(mu)\n",
    "\n",
    "m1 = m1[indices]\n",
    "m2 = m2[indices]\n",
    "mu = mu[indices]\n",
    "C = C[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480604ea",
   "metadata": {},
   "source": [
    "## Sparse accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e437b",
   "metadata": {},
   "source": [
    "cpu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70fbe0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 42])\n"
     ]
    }
   ],
   "source": [
    "output = accumulate(X1, X2, mu, 42, m1, m2, C)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ebc863",
   "metadata": {},
   "source": [
    "Let's move our dummy data to the gpu:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ff70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_cuda = X1.cuda()\n",
    "X2_cuda = X2.cuda()\n",
    "m1_cuda = m1.cuda()\n",
    "m2_cuda = m2.cuda()\n",
    "mu_cuda = mu.cuda()\n",
    "C_cuda = C.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f0299",
   "metadata": {},
   "source": [
    "gpu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b646ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 42])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "output = accumulate(X1_cuda, X2_cuda, mu_cuda, 42, m1_cuda, m2_cuda, C_cuda)\n",
    "print(output.shape)\n",
    "print(output.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d4700",
   "metadata": {},
   "source": [
    "## [optional] Clebsch-Gordan iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89ad82",
   "metadata": {},
   "source": [
    "If we want the sparse accumulation operation to do the actual Clebsch-Gordan iteration we need to precompute the corresponding transformation rule and populate the arrays ``m1``, ``m2``, ``mu`` and ``C`` with the actual Clebsch-Gordan coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "423cb660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([93]) torch.Size([93]) torch.Size([93]) torch.Size([93])\n"
     ]
    }
   ],
   "source": [
    "l1 = 3\n",
    "l2 = 4\n",
    "l_output = 5\n",
    "m1, m2, mu, C = get_cg_transformation_rule(l1, l2, l_output)\n",
    "print(m1.shape, m2.shape, mu.shape, C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a219bc6",
   "metadata": {},
   "source": [
    "The mentioned above sorting operation is not required now since it has been already performed inside `get_cg_transformation_rule`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5b470",
   "metadata": {},
   "source": [
    "Now, given this transformation rule, sparse accumulation operation performs actual CG iteration, producing \n",
    "covariant vectors with l = l_output given covariant vectors with l = l1 and l = l2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61d89904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 11])\n"
     ]
    }
   ],
   "source": [
    "X1 = torch.randn(10, 20, 2 * l1 + 1)\n",
    "X2 = torch.randn(10, 20, 2 * l2 + 1)\n",
    "output = accumulate(X1, X2, mu, 2 * l_output + 1, m1, m2, C)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97342a",
   "metadata": {},
   "source": [
    "## Outdated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69963ab",
   "metadata": {},
   "source": [
    "The goal is to compute this for all $\\mu$:\n",
    "\n",
    "$\\text{Output}[:, :, \\mu] = \\sum\\limits_{m_1, m_2} \\text{X_1}[:, :, m_1] * \\text{X_2}[:, :, m_2] * C_{m_1, m_2, \\mu}$\n",
    "\n",
    "This is the subpart of the Clebsch-Gordan iteration for fixed l1, l2, and l. The first two dimensions are the \"dense\" ones, so the same operation is performed for all the indices in the first two dimensions. \n",
    "\n",
    "Since Clebsch-Gordan coefficients are very sparse, it is worthwhile to align them into a 1-dimensional tensor containing only non-zero values, but in this case, we need to supply this tensor with supplementary indices tensors telling us what are the corresponding m1, m2, and $\\mu$ indices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d56ce1",
   "metadata": {},
   "source": [
    "Reference slow python implementation is as simple as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d8e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_accumulation_loops(X1, X2, idx_output, output_size, idx_1, idx_2, multipliers):\n",
    "    device = X1.device #all tensors must be on the same device and blah, blah, blah    \n",
    "    dtype = X1.dtype    \n",
    "    \n",
    "    output = torch.zeros([X1.shape[0], X2.shape[1], output_size], device = device,dtype=dtype)\n",
    "    for index in range(idx_output.shape[0]):       \n",
    "        output[:, :, idx_output[index]] += X1[:, :, idx_1[index]] * X2[:, :, idx_2[index]] * multipliers[index]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27d3b2",
   "metadata": {},
   "source": [
    "Here multipliers are the values of Clebsch-Gordan coefficients, idx_1 is the tensor containing corresponding m1 indices, idx_2 is the tensor containing corresponding m2 indices, and idx_output is the tensor containing $\\mu$ indices. output_size is just a single integer, the desired length of the output (2 * l + 1). \n",
    "\n",
    "So the loops go over all the terms, for all $\\mu$, m1, and m2 with non-zero clebsch-gordan coefficients, and the current contribution is added to the output array to the proper place defined by $\\mu$ which is stored in the idx_output\n",
    "\n",
    "The first two dense dimensions are introduced, keeping in mind batch and feature dimensions. If you need just 1, it is possible to introduce a dummy dimension of size 1 ^^. \n",
    "\n",
    "\n",
    "The transformation itself, i.e., Clebsch-Gordan coefficients, can be precomputed once at the very beginning. This repo among the other things contains the code for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d0ab81",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clebsch_gordan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclebsch_gordan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClebschGordan, get_real_clebsch_gordan\n\u001b[1;32m      2\u001b[0m L_MAX \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      3\u001b[0m clebsch \u001b[38;5;241m=\u001b[39m ClebschGordan(L_MAX)\u001b[38;5;241m.\u001b[39mprecomputed_\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clebsch_gordan'"
     ]
    }
   ],
   "source": [
    "from clebsch_gordan import ClebschGordan, get_real_clebsch_gordan\n",
    "L_MAX = 5\n",
    "clebsch = ClebschGordan(L_MAX).precomputed_\n",
    "indices = get_real_clebsch_gordan(clebsch[L_MAX, L_MAX, L_MAX], L_MAX, L_MAX, L_MAX)\n",
    "\n",
    "m1_aligned, m2_aligned = [], []\n",
    "multipliers, mu_aligned = [], []\n",
    "for mu in range(0, 2 * L_MAX + 1):\n",
    "    for el in indices[mu]:\n",
    "        m1, m2, multiplier = el\n",
    "        m1_aligned.append(m1)\n",
    "        m2_aligned.append(m2)\n",
    "        multipliers.append(multiplier)\n",
    "        mu_aligned.append(mu)\n",
    "m1_aligned = torch.LongTensor(m1_aligned)\n",
    "m2_aligned = torch.LongTensor(m2_aligned)\n",
    "mu_aligned = torch.LongTensor(mu_aligned)\n",
    "multipliers = torch.FloatTensor(multipliers)\n",
    "\n",
    "indices = np.argsort(mu_aligned)\n",
    "\n",
    "m1_aligned = m1_aligned[indices].cuda()\n",
    "m2_aligned = m2_aligned[indices].cuda()\n",
    "mu_aligned = mu_aligned[indices].cuda()\n",
    "multipliers = multipliers[indices].cuda()\n",
    "\n",
    "print(\"L_MAX: \")\n",
    "print(\"multipliers shape: \", multipliers.shape)\n",
    "print(\"m1_aligned shape: \", m1_aligned.shape)\n",
    "print(\"m2_aligned shape: \", m2_aligned.shape)\n",
    "print(\"multipliers shape: \", multipliers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15db4ef",
   "metadata": {},
   "source": [
    "This is a simple wrapper on sympy package, and the definition of the real clebsch-gordan coefficients is consistent with librascal real spherical harmonics, nice, wigner iterations, and rascaline\n",
    "\n",
    "Now we can do the Clebsch-Gordan iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee0c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = torch.randn(100, 17, 2 * L_MAX + 1).cuda()\n",
    "X2 = torch.randn(100, 17, 2 * L_MAX + 1).cuda()\n",
    "\n",
    "output_loops = sparse_accumulation_loops(X1, X2, mu_aligned, 2 * L_MAX + 1, m1_aligned, m2_aligned, multipliers)\n",
    "print(output_loops.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d475e",
   "metadata": {},
   "source": [
    "You can take a look at the benchmarks files .py along with their output .out to get an idea 1) how to benchmark this properly with gpu synchronization and 2) the speed of this operation compared to a naive implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8c1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
